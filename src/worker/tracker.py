import asyncio
import logging
import httpx
from datetime import datetime
from sqlalchemy import select, func, case
from src.database import async_session_maker, init_db
from src.models import Pool, Transaction, Wallet
from src.config import settings
from src.worker.transactions import TransactionProcessor
from src.worker.sse import SSEMonitor

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)



class JettonTracker:
    def __init__(self):
        self.is_running = False
        self.delay = 1.0 / settings.requests_per_second
        self.batch_size = settings.worker_batch_size
        self.processor = TransactionProcessor()
        self.sse_monitors = []
    
    async def start(self):
        logger.info(f"üöÄ Starting Jetton Tracker...")
        logger.info(f"Rate limit: {settings.requests_per_second} req/sec")
        logger.info(f"Batch size: {self.batch_size}")
        
        await init_db()
        await self.ensure_leaderboard_ready()
        await self.start_sse_monitors()
        await self.initial_pool_sync()
        
        self.is_running = True
        
        while self.is_running:
            try:
                synced = await self.sync_new_wallets()
                
                processed = await self.process_pending_transactions()
                
                if synced == 0 and processed == 0:
                    await asyncio.sleep(5)
                
            except Exception as e:
                logger.error(f"Error in tracker loop: {e}")
                await asyncio.sleep(10)
    
    async def stop(self):
        logger.info("Stopping Jetton tracker...")
        self.is_running = False
        
        for monitor in self.sse_monitors:
            monitor.stop()
        
        await self.processor.close()
    
    async def start_sse_monitors(self):
        async with async_session_maker() as db:
            result = await db.execute(
                select(Pool).where(Pool.is_active == True)
            )
            pools = result.scalars().all()
            
            if not pools:
                logger.warning("‚ö†Ô∏è  No active pools found!")
                return
            
            for pool in pools:
                monitor = SSEMonitor(pool)
                self.sse_monitors.append(monitor)
                asyncio.create_task(monitor.start())
                logger.info(f"Started SSE monitor for pool: {pool.name or pool.address[:8]}")
    
    async def initial_pool_sync(self):
        logger.info("üîÑ Starting initial pool sync...")
        sync_start_time = asyncio.get_event_loop().time()
        
        async with async_session_maker() as db:
            result = await db.execute(select(Pool).where(Pool.is_active == True))
            pools = result.scalars().all()
            
            if not pools:
                logger.warning("No active pools found for sync")
                return
            
            for pool in pools:
                pool_sync_start = asyncio.get_event_loop().time()
                
                # üöÄ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –µ—Å–ª–∏ –µ—Å—Ç—å last_processed_lt, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –ù–û–í–´–• —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
                if pool.last_processed_lt:
                    logger.info(f"üìÖ Pool {pool.name}: fetching NEW transactions after LT {pool.last_processed_lt}")
                    all_txs = await self.fetch_pool_transactions(
                        pool_address=pool.address,
                        after_lt=pool.last_processed_lt
                    )
                else:
                    # –ò–Ω–∞—á–µ –Ω–∞—á–∏–Ω–∞–µ–º —Å START_DATE (–ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫)
                    start_timestamp = int(datetime.strptime(settings.start_date, "%Y-%m-%d").timestamp())
                    logger.info(f"üìÖ Pool {pool.name}: first sync - fetching transactions from {settings.start_date}")
                    all_txs = await self.fetch_pool_transactions(
                        pool_address=pool.address,
                        start_timestamp=start_timestamp
                    )
                
                try:
                    added = 0
                    skipped = 0
                    logger.info(f"üîç Filtering {len(all_txs)} transactions with parallel batch processing...")
                    
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º last_processed_lt –≤ —á–∏—Å–ª–æ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                    stop_at_lt = int(pool.last_processed_lt) if pool.last_processed_lt else 0
                    if stop_at_lt > 0:
                        logger.info(f"‚õî Will stop processing at LT {stop_at_lt} (previously synced)")
                    
                    max_added_lt = 0  # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π LT —Ç–æ–ª—å–∫–æ –¥–ª—è –î–û–ë–ê–í–õ–ï–ù–ù–´–• —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ —Å–æ–≥–ª–∞—Å–Ω–æ RPS
                    max_concurrent_checks = 2  # –ú–∞–∫—Å–∏–º—É–º 2 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –æ—Ç 429
                    filter_semaphore = asyncio.Semaphore(max_concurrent_checks)
                    logger.info(f"‚è±Ô∏è  Using {max_concurrent_checks} concurrent checks for LAMBO verification")
                    
                    async def check_lambo_with_limit(tx_hash, jetton_master):
                        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç LAMBO —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–æ —Å–µ–º–∞—Ñ–æ—Ä—É"""
                        async with filter_semaphore:
                            return await self.processor.is_lambo_transaction(tx_hash, jetton_master)
                    
                    filter_batch_size = max_concurrent_checks * 2  # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
                    
                    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                    for batch_idx in range(0, len(all_txs), filter_batch_size):
                        batch = all_txs[batch_idx:batch_idx + filter_batch_size]
                        batch_progress = min(batch_idx + filter_batch_size, len(all_txs))
                        
                        if batch_progress % 100 == 0 or batch_progress == len(all_txs):
                            logger.info(f"   Progress: {batch_progress}/{len(all_txs)}, added: {added}, skipped: {skipped}")
                        
                        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
                        check_tasks = []
                        for tx_data in batch:
                            tx_lt = int(tx_data["lt"])
                            
                            # –ï—Å–ª–∏ –≤—Å—Ç—Ä–µ—Ç–∏–ª–∏ —Ä–∞–Ω–µ–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π LT, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
                            if stop_at_lt > 0 and tx_lt <= stop_at_lt:
                                logger.info(f"‚õî Reached previously synced LT {tx_lt}, stopping batch processing")
                                break
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç –≤ –ë–î
                            existing = await db.execute(
                                select(Transaction).where(Transaction.tx_hash == tx_data["hash"])
                            )
                            if not existing.scalar_one_or_none():
                                # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ LAMBO —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è (—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º)
                                check_tasks.append((tx_data, check_lambo_with_limit(tx_data["hash"], pool.jetton_master)))
                        
                        # –ï—Å–ª–∏ –≤—Å—Ç—Ä–µ—Ç–∏–ª–∏ —Ä–∞–Ω–µ–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π LT, –≤—ã—Ö–æ–¥–∏–º –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
                        if check_tasks and batch and int(batch[-1]["lt"]) <= stop_at_lt:
                            logger.info(f"üìç Batch contains previously synced data, finishing...")
                        
                        # –ñ–¥–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
                        if check_tasks:
                            results = await asyncio.gather(*[task for _, task in check_tasks], return_exceptions=True)
                            
                            for (tx_data, _), is_lambo_result in zip(check_tasks, results):
                                if isinstance(is_lambo_result, Exception):
                                    logger.error(f"Error checking tx {tx_data['hash']}: {is_lambo_result}")
                                    skipped += 1
                                    continue
                                
                                if not is_lambo_result:
                                    skipped += 1
                                    continue
                                
                                tx = Transaction(
                                    tx_hash=tx_data["hash"],
                                    lt=str(tx_data["lt"]),
                                    timestamp=tx_data["utime"],
                                    pool_id=pool.id,
                                    is_processed=False
                                )
                                db.add(tx)
                                added += 1
                                max_added_lt = max(max_added_lt, int(tx_data["lt"]))
                        
                        # Commit –∫–∞–∂–¥—ã–µ 50 –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
                        if added % 50 == 0:
                            await db.commit()
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π LT —á—Ç–æ–±—ã –∑–Ω–∞—Ç—å –≥–¥–µ –æ—Å—Ç–∞–Ω–æ–≤–∏–ª–∏—Å—å
                            if max_added_lt > stop_at_lt:
                                pool.last_processed_lt = str(max_added_lt)
                                pool.last_sync_timestamp = int(datetime.utcnow().timestamp())
                                await db.commit()
                                logger.info(f"üíæ Intermediate checkpoint: saved LT {max_added_lt} after {added} transactions")
                                max_added_lt = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –±–∞—Ç—á–∞
                    
                    await db.commit()
                    logger.info(f"üìä Filtered: added {added} LAMBO txs, skipped {skipped} non-LAMBO")
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º last_processed_lt —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–æ–±–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
                    # –∏–ª–∏ –µ—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
                    if added > 0 and all_txs:
                        # –ù–∞—Ö–æ–¥–∏–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π LT —Å—Ä–µ–¥–∏ –Ω–æ–≤—ã—Ö –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö (–Ω–µ –≤—Å–µ—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö)
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º LT –ø–µ—Ä–≤–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∫–æ—Ç–æ—Ä–∞—è –±—ã–ª–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∞
                        latest_lt = max(int(tx["lt"]) for tx in all_txs if int(tx["lt"]) > stop_at_lt)
                        pool.last_processed_lt = str(latest_lt)
                        pool.last_sync_timestamp = int(datetime.utcnow().timestamp())
                        await db.commit()
                        logger.info(f"‚úÖ Pool {pool.name}: synced to LT {latest_lt}, added {added} new transactions")
                    elif stop_at_lt > 0:
                        # –£–∂–µ –±—ã–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã —Ä–∞–Ω–µ–µ
                        logger.info(f"‚úÖ Pool {pool.name}: no new transactions (already synced up to LT {stop_at_lt})")
                    else:
                        logger.info(f"‚úÖ Pool {pool.name}: no new transactions")
                    
                    pool_sync_time = asyncio.get_event_loop().time() - pool_sync_start
                    logger.info(f"‚è±Ô∏è  Pool sync time: {pool_sync_time:.2f}s")
                    
                except Exception as e:
                    logger.error(f"‚ùå Error syncing pool {pool.name}: {e}")
        
        total_sync_time = asyncio.get_event_loop().time() - sync_start_time
        logger.info(f"‚úÖ Initial pool sync complete (total time: {total_sync_time:.2f}s)")
    
    async def fetch_pool_transactions(self, pool_address: str, start_timestamp: int = None, after_lt: str = None) -> list:
        url = f"{settings.ton_api_url}/v2/blockchain/accounts/{pool_address}/transactions"
        all_transactions = []
        max_concurrent_requests = 10  # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –¥–æ 10 –∑–∞–ø—Ä–æ—Å–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
        target_rps = settings.requests_per_second  # –¶–µ–ª–µ–≤–æ–π RPS (10)
        
        if after_lt:
            logger.info(f"Fetching new transactions after LT {after_lt}")
        
        logger.info(f"üöÄ Starting parallel fetch - Target: {target_rps} RPS, Initial concurrent: {max_concurrent_requests}")
        
        async def fetch_page(client, before_lt=None):
            """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π"""
            params = {"limit": 1000}
            if before_lt:
                params["before_lt"] = before_lt
            
            request_start = asyncio.get_event_loop().time()
            try:
                response = await client.get(url, params=params)
                response.raise_for_status()
                request_time = asyncio.get_event_loop().time() - request_start
                data = response.json()
                transactions = data.get("transactions", [])
                return transactions, request_time
            except Exception as e:
                logger.error(f"Error fetching page: {e}")
                return [], 0
        
        async with httpx.AsyncClient(
            timeout=30.0,
            headers={'Authorization': f'Bearer {settings.ton_api_key}'}
        ) as client:
            semaphore = asyncio.Semaphore(max_concurrent_requests)
            
            async def fetch_with_semaphore(before_lt=None):
                async with semaphore:
                    return await fetch_page(client, before_lt)
            
            # –ù–∞—á–∏–Ω–∞–µ–º —Å –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            first_result = await fetch_page(client, after_lt)
            if isinstance(first_result, tuple):
                first_page, _ = first_result
            else:
                first_page = first_result
                
            if not first_page:
                logger.info("No transactions found")
                return []
            
            all_transactions.extend(first_page)
            
            # –ï—Å–ª–∏ –≤–µ—Ä–Ω—É–ª–æ—Å—å –º–µ–Ω—å—à–µ limit, —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
            if len(first_page) < 1000:
                logger.info("First page has less than limit, no more pages")
                return all_transactions
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
            current_before_lt = str(first_page[-1]["lt"])
            
            fetch_start = asyncio.get_event_loop().time()
            total_requests = 1  # –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å —É–∂–µ –±—ã–ª
            batch_count = 0
            request_times = []
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ, –ø—Ä–æ–≤–µ—Ä—è—è —É—Å–ª–æ–≤–∏—è
            while True:
                batch_count += 1
                batch_start = asyncio.get_event_loop().time()
                
                # –°–æ–∑–¥–∞–µ–º –±–∞—Ç—á –∏–∑ max_concurrent_requests –∑–∞–¥–∞—á
                batch = []
                for _ in range(max_concurrent_requests):
                    task = asyncio.create_task(fetch_with_semaphore(current_before_lt))
                    batch.append(task)
                
                # –ñ–¥–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–∞—Ç—á–∞
                results = await asyncio.gather(*batch, return_exceptions=True)
                batch_time = asyncio.get_event_loop().time() - batch_start
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —É—Å–ø–µ—à–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∏ –≤—Ä–µ–º—è
                successful_requests = 0
                batch_requests_time = []
                for result in results:
                    if isinstance(result, tuple) and len(result) == 2:
                        transactions, req_time = result
                        if transactions:  # –£—Å–ø–µ—à–Ω—ã–π –∑–∞–ø—Ä–æ—Å
                            successful_requests += 1
                            batch_requests_time.append(req_time)
                            request_times.append(req_time)
                    elif isinstance(result, Exception):
                        logger.error(f"Task error: {result}")
                
                total_requests += successful_requests
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Ç–µ–∫—É—â–∏–π RPS –∑–∞ —ç—Ç–æ—Ç –±–∞—Ç—á
                current_rps = successful_requests / max(batch_time, 0.1)
                
                # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ: –µ—Å–ª–∏ RPS < —Ü–µ–ª–µ–≤–æ–≥–æ, –ø—Ä–æ–±—É–µ–º —É–≤–µ–ª–∏—á–∏—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º
                if current_rps < target_rps * 0.9 and max_concurrent_requests < 30:
                    old_max = max_concurrent_requests
                    max_concurrent_requests = min(max_concurrent_requests + 3, 30)
                    logger.info(f"‚ö° Increasing concurrent requests: {old_max} ‚Üí {max_concurrent_requests} (current RPS: {current_rps:.1f})")
                elif current_rps > target_rps * 1.1 and max_concurrent_requests > 5:
                    old_max = max_concurrent_requests
                    max_concurrent_requests = max(max_concurrent_requests - 2, 5)
                    logger.info(f"‚¨áÔ∏è  Decreasing concurrent requests: {old_max} ‚Üí {max_concurrent_requests} (current RPS: {current_rps:.1f})")
                
                batch_has_data = False
                for result in results:
                    if isinstance(result, tuple):
                        transactions, _ = result
                        if transactions:
                            all_transactions.extend(transactions)
                            batch_has_data = True
                            
                            # –û–±–Ω–æ–≤–ª—è–µ–º before_lt –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                            if len(transactions) >= 1000:
                                current_before_lt = str(transactions[-1]["lt"])
                            else:
                                # –ï—Å–ª–∏ –≤ —ç—Ç–æ–º –±–∞—Ç—á–µ –±—ã–ª–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞, –≤—ã—Ö–æ–¥–∏–º
                                logger.info(f"Reached last page in batch, total: {len(all_transactions)}")
                                fetch_time = asyncio.get_event_loop().time() - fetch_start
                                actual_rps = total_requests / max(fetch_time, 0.1)
                                logger.info(f"üìä Parallel fetch complete: {len(all_transactions)} tx from {total_requests} requests in {fetch_time:.2f}s (avg RPS: {actual_rps:.1f})")
                                return all_transactions
                    elif isinstance(result, Exception):
                        logger.error(f"Task error: {result}")
                
                if not batch_has_data:
                    logger.info(f"No more data in batch, total: {len(all_transactions)}")
                    break
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ª–æ–≤–∏–µ start_timestamp –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
                if not after_lt and start_timestamp and all_transactions:
                    last_tx_time = int(all_transactions[-1]["utime"])
                    if last_tx_time <= start_timestamp:
                        logger.info(f"Reached start_timestamp {start_timestamp}, stopping")
                        break
        
        fetch_time = asyncio.get_event_loop().time() - fetch_start
        actual_rps = total_requests / max(fetch_time, 0.1)
        logger.info(f"üìä Parallel fetch complete: {len(all_transactions)} tx from {total_requests} requests in {fetch_time:.2f}s (avg RPS: {actual_rps:.1f})")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –ø–æ start_timestamp (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ after_lt –Ω–µ —É–∫–∞–∑–∞–Ω)
        if not after_lt and start_timestamp:
            all_transactions = [tx for tx in all_transactions if int(tx["utime"]) >= start_timestamp]
            logger.info(f"After filtering by timestamp: {len(all_transactions)} transactions")
        
        return all_transactions
    
    async def sync_new_wallets(self) -> int:
        async with async_session_maker() as db:
            result = await db.execute(
                select(Wallet).where(
                    Wallet.sync_status == 'pending',
                    Wallet.is_active == True
                ).limit(5)
            )
            wallets = result.scalars().all()
            
            if not wallets:
                return 0
            
            logger.info(f"üîÑ Syncing {len(wallets)} new wallets...")
            
            for wallet in wallets:
                wallet.sync_status = 'syncing'
                await db.commit()
                
                result = await db.execute(
                    select(
                        func.coalesce(func.sum(case((Transaction.operation_type == 'buy', Transaction.lambo_amount), else_=0)), 0).label('buy_lambo'),
                        func.coalesce(func.sum(case((Transaction.operation_type == 'buy', Transaction.ton_amount), else_=0)), 0).label('buy_ton'),
                        func.coalesce(func.sum(case((Transaction.operation_type == 'buy', Transaction.ton_amount * Transaction.ton_usd_price), else_=0)), 0).label('buy_usd'),
                        func.coalesce(func.sum(case((Transaction.operation_type == 'sell', Transaction.lambo_amount), else_=0)), 0).label('sell_lambo'),
                        func.coalesce(func.sum(case((Transaction.operation_type == 'sell', Transaction.ton_amount), else_=0)), 0).label('sell_ton'),
                        func.coalesce(func.sum(case((Transaction.operation_type == 'sell', Transaction.ton_amount * Transaction.ton_usd_price), else_=0)), 0).label('sell_usd'),
                        func.count(Transaction.id).label('tx_count')
                    )
                    .where(
                        Transaction.user_address == wallet.address,
                        Transaction.is_processed == True
                    )
                )
                
                volumes = result.one()
                
                wallet.buy_volume_lambo = float(volumes.buy_lambo)
                wallet.buy_volume_ton = float(volumes.buy_ton)
                wallet.buy_volume_usd = float(volumes.buy_usd)
                wallet.sell_volume_lambo = float(volumes.sell_lambo)
                wallet.sell_volume_ton = float(volumes.sell_ton)
                wallet.sell_volume_usd = float(volumes.sell_usd)
                wallet.total_volume_lambo = wallet.buy_volume_lambo + wallet.sell_volume_lambo
                wallet.total_volume_ton = wallet.buy_volume_ton + wallet.sell_volume_ton
                wallet.total_volume_usd = wallet.buy_volume_usd + wallet.sell_volume_usd
                
                wallet.sync_status = 'synced'
                wallet.initial_sync_completed = True
                
                await db.commit()
                
                from src.services.leaderboard_service import update_leaderboard
                update_leaderboard(wallet.address, wallet.total_volume_usd)
                
                logger.info(
                    f"‚úÖ Synced {wallet.address[:8]}... "
                    f"from {volumes.tx_count} tx, "
                    f"USD: ${wallet.total_volume_usd:.2f}"
                )
            
            return len(wallets)
    
    async def process_pending_transactions(self) -> int:
        async with async_session_maker() as db:
            result = await db.execute(
                select(Transaction)
                .where(Transaction.is_processed == False)
                .order_by(Transaction.timestamp.asc())
                .limit(self.batch_size)
            )
            transactions = result.scalars().all()
            
            if not transactions:
                return 0
            
            logger.info(f"üîß Processing {len(transactions)} pending transactions...")
            
            processed_count = 0
            for tx in transactions:
                try:
                    success = await self.processor.process_transaction(tx, db)
                    if success:
                        processed_count += 1
                    
                    await asyncio.sleep(self.delay)
                    
                except Exception as e:
                    logger.error(f"Error processing tx {tx.tx_hash}: {e}")
                    await asyncio.sleep(self.delay)
            
            if processed_count > 0:
                logger.info(f"‚úÖ Successfully processed {processed_count}/{len(transactions)} transactions")
            
            return len(transactions)
    
    async def ensure_leaderboard_ready(self):
        try:
            from src.services.leaderboard_service import get_total_wallets
            
            total = get_total_wallets()
            
            if total == 0:
                logger.warning("‚ö†Ô∏è  Redis leaderboard is empty! Rebuilding from database...")
                async with async_session_maker() as db:
                    from src.services.leaderboard_service import rebuild_leaderboard_from_db
                    result = await rebuild_leaderboard_from_db(db)
                    if result.get("rebuilt"):
                        logger.info(f"‚úÖ Leaderboard initialized: {result}")
                    else:
                        logger.warning(f"‚ö†Ô∏è  Failed to rebuild leaderboard: {result}")
            else:
                logger.info(f"‚úÖ Redis leaderboard ready: {total} wallets")
        except Exception as e:
            logger.error(f"‚ùå Error ensuring leaderboard ready: {e}")


async def main():
    tracker = JettonTracker()
    
    try:
        await tracker.start()
    except KeyboardInterrupt:
        logger.info("Received shutdown signal")
        await tracker.stop()


if __name__ == "__main__":
    asyncio.run(main())
